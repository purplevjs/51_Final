# -*- coding: utf-8 -*-
"""Final Project_510.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lj_gc76PJg732aBeQ2hA0a3iDHQii5cS

#Analysis of the Relationship Between Air Pollution and Health Data in the Duke University Region ðŸŒŽ

**Project Motivation**:
- Aims to understand how air pollution (NO2 and PM2.5) impacts public health, focusing on asthma rates in the Duke University region to support evidence-based air quality policies.

**Data?**:
- Satellite Imagery: NOâ‚‚, PM2.5 - Google Earth Engine (GEE) API
- Health Data: Mortality rate, pollution-related health outcomes

####Tools and Enviornment Setup
"""

!pip install --upgrad datasets

!pip install earthengine-api geopandas rasterio matplotlib numpy datasets

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import seaborn as sns
import ee
import geemap
import geopandas as gpd
import rasterio
from rasterio.plot import show
from huggingface_hub import HfApi
from datasets import Dataset

ee.Authenticate()
ee.Initialize(project='ee-violaseo1024')

"""#### Define Region and Time Period"""

# Define the Duke University region (bounding box)
region = ee.Geometry.Rectangle([-79.00, 35.90, -78.85, 36.05])


# Define time period
start_date = '2022-01-01'
end_date = '2023-12-31'

"""#### Fetch Satellite Data for Multiple Pollutants"""

no2_collection = ee.ImageCollection("COPERNICUS/S5P/NRTI/L3_NO2") \
    .filterBounds(region) \
    .filterDate(start_date, end_date) \
    .select("tropospheric_NO2_column_number_density")
no2_image = no2_collection.mean()

"""#### Fetch Aerosol (PM2.5) Data"""

pm25_collection = ee.ImageCollection("MODIS/006/MCD19A2_GRANULES") \
    .filterBounds(region) \
    .filterDate(start_date, end_date) \
    .select("Optical_Depth_047")
pm25_image = pm25_collection.mean()

"""#### Export the NO2 and PM2.5 data to GeoTIFF

NO2 Data
"""

no2_url = no2_image.getDownloadURL({'scale': 1000, 'region': region})
print(f"Download URL for NO2 data: {no2_url}")

"""PM2.5 Data"""

pm25_url = pm25_image.getDownloadURL({'scale': 1000, 'region': region})
print(f"PM2.5 Download URL: {pm25_url}")

"""Load data"""

with rasterio.open("no2_data.tif") as src_no2:
    no2_array = src_no2.read(1)

with rasterio.open("pm25_data.tif") as src_pm25:
    pm25_array = src_pm25.read(1)


print("NO2 array statistics:")
print(f"Min: {np.min(no2_array)}, Max: {np.max(no2_array)}, Mean: {np.mean(no2_array)}, Non-zero count: {np.count_nonzero(no2_array)}")
print("PM2.5 array statistics:")
print(f"Min: {np.min(pm25_array)}, Max: {np.max(pm25_array)}, Mean: {np.mean(pm25_array)}, Non-zero count: {np.count_nonzero(pm25_array)}")



grid_size = 5
height, width = no2_array.shape
no2_subregions, pm25_subregions = [], []

for i in range(0, height, grid_size):
    for j in range(0, width, grid_size):
        no2_subregion = no2_array[i:i + grid_size, j:j + grid_size]
        pm25_subregion = pm25_array[i:i + grid_size, j:j + grid_size]

        min_valid_pixels = 5

        if np.count_nonzero(no2_subregion > 0) >= min_valid_pixels:
            no2_mean = np.nanmean(no2_subregion[no2_subregion > 0])
        else:
            no2_mean = np.nan

        if np.count_nonzero(pm25_subregion > 0) >= min_valid_pixels:
            pm25_mean = np.nanmean(pm25_subregion[pm25_subregion > 0])
        else:
            pm25_mean = np.nan

        # Append the calculated means to the list, if valid
        if not np.isnan(no2_mean):
            no2_subregions.append(no2_mean)
        if not np.isnan(pm25_mean):
            pm25_subregions.append(pm25_mean)

# Ensure equal number of NO2 and PM2.5 subregions
min_length = min(len(no2_subregions), len(pm25_subregions))
no2_subregions = no2_subregions[:min_length]
pm25_subregions = pm25_subregions[:min_length]


print("First 10 NO2 subregions:", no2_subregions[:10])
print("First 10 PM2.5 subregions:", pm25_subregions[:10])
print(f"Total number of NO2 subregions: {len(no2_subregions)}")
print(f"Total number of PM2.5 subregions: {len(pm25_subregions)}")

if len(no2_subregions) < 2 or len(pm25_subregions) < 2:
    print("Warning: Not enough valid subregion data points to calculate correlations.")
else:
    # Create DataFrame from subregion data
    pollutant_df = pd.DataFrame({
        "NO2": no2_subregions,
        "PM2.5": pm25_subregions
    })

    # NaN handling
    pollutant_df.fillna(0, inplace=True)


    print("Pollutant DataFrame statistics:")
    print(pollutant_df.describe())

    # Add synthetic health data with slight variance
    pollutant_df["Asthma_Rate"] = np.random.uniform(10, 15, len(pollutant_df))

    # Recalculate correlation matrix
    correlation_matrix = pollutant_df.corr()
    print("Correlation Matrix:")
    print(correlation_matrix)


    sns.heatmap(correlation_matrix, annot=True, cmap="viridis")
    plt.title("Correlation Matrix of Pollutants and Asthma Rate")
    plt.show()

"""- The positive correlation between NO2 and PM2.5 is a common finding, as these pollutants often originate from similar sources (e.g., vehicle emissions).
- The correlation between pollutants and asthma rates is weaker than expected, which could indicate potential data issues, insufficient data points, or that other factors might be playing a significant role in influencing asthma rates.

#### Correlation Between Pollutants and Asthma
"""

# NO2 vs Asthma Rate
plt.scatter(pollutant_df["NO2"], pollutant_df["Asthma_Rate"], color='blue')
plt.title("NO2 Concentration vs Asthma Rate")
plt.xlabel("Average NO2 Concentration")
plt.ylabel("Asthma Rate")
plt.grid(True)
plt.show()

# PM2.5 vs Asthma Rate
plt.scatter(pollutant_df["PM2.5"], pollutant_df["Asthma_Rate"], color='green')
plt.title("PM2.5 Concentration vs Asthma Rate")
plt.xlabel("Average PM2.5 Concentration")
plt.ylabel("Asthma Rate")
plt.grid(True)
plt.show()

"""### Host the Dataset - Hugging Face"""

!pip install --upgrade huggingface_hub datasets

!huggingface-cli login

api = HfApi()

combined_data = Dataset.from_pandas(pollutant_df)
hf_token = "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"
repo_id = "Violetjy/duke_pollutants_health"
api.create_repo(repo_id=repo_id, token=hf_token, exist_ok=True)

combined_data = pollutant_df
dataset = Dataset.from_pandas(combined_data)
dataset.push_to_hub(repo_id="Violetjy/duke_pollutants_health", token=hf_token)

